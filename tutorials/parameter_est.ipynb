{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nafshordi/Mystery-noise/blob/master/tutorials/parameter_est.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0NglCcdXGis"
      },
      "source": [
        "## Run this notebook in Google Colab by clicking here: [Google Colab](https://colab.research.google.com/github/nanograv/15yr_stochastic_analysis/blob/master/tutorials/parameter_est.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pU4IesQXGis"
      },
      "source": [
        "### Run these cells if using Colab. Otherwise, skip them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AwjVrHFdXGit",
        "outputId": "472231fc-9f28-4a54-b25b-5616983ae758",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Mambaforge has been sunset. It is now identical to Miniforge. Installing Miniforge...\n"
          ]
        }
      ],
      "source": [
        "# This cell will reset the kernel.\n",
        "# Run this cell, wait until it's done, then run the next.\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install_mambaforge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lXzxPgtOXGit"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mamba install -y -c conda-forge enterprise_extensions la_forge ipympl\n",
        "!git clone https://github.com/nanograv/15yr_stochastic_analysis\n",
        "import sys\n",
        "sys.path.insert(0,'/content/15yr_stochastic_analysis/tutorials')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp9RT689XGit"
      },
      "source": [
        "# Using `enterprise` to perform parameter estimation on an isotropic stochastic gravitational wave background with the NANOGrav PTA\n",
        "\n",
        "In this notebook you will learn:\n",
        "* How to use `enterprise` to interact with NANOGrav data,\n",
        "\n",
        "* How to search the full NANOGrav PTA for a common red noise process,\n",
        "\n",
        "* How to perform parameter estimation on the NANOGrav 15-year data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W4bQ_OLXGit"
      },
      "source": [
        "# Load packages and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HhVOjZa6XGit",
        "outputId": "894119b7-83a2-4540-f04c-964838c0858b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-435058316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-57>\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/extensions/autoreload.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msource_from_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m#------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from enterprise.signals import parameter\n",
        "from enterprise.signals import utils\n",
        "from enterprise.signals import signal_base\n",
        "from enterprise.signals import selections\n",
        "from enterprise.signals import white_signals\n",
        "from enterprise.signals import gp_signals\n",
        "\n",
        "from extra_functions import EmpiricalDistribution2D\n",
        "\n",
        "from enterprise_extensions.models import model_general\n",
        "from enterprise_extensions import hypermodel\n",
        "from enterprise_extensions import sampler as ee_sampler\n",
        "\n",
        "from PTMCMCSampler.PTMCMCSampler import PTSampler as ptmcmc\n",
        "\n",
        "from la_forge import core, diagnostics\n",
        "\n",
        "from enterprise_extensions.load_feathers import load_feathers_from_folder\n",
        "\n",
        "import corner\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQuaCq4HXGit"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    datadir = '/content/15yr_stochastic_analysis/tutorials/data'\n",
        "else:\n",
        "    datadir = './data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdG9d8ifXGit"
      },
      "source": [
        "## Load the full set of Pulsar objects\n",
        "\n",
        "  * These files have been stored as `feather` files to make them much faster to load (and take up little space)\n",
        "  \n",
        "  * See the `explore_data.ipynb` tutorial to see what exists in these files and how to load `.par` and `.tim` files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0t_M_3RXGit"
      },
      "outputs": [],
      "source": [
        "psrs = load_feathers_from_folder(datadir + '/feathers')\n",
        "print('Loaded {0} pulsars from feather files'.format(len(psrs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9ufXRGGXGiu"
      },
      "source": [
        "## Read in white noise dictionaries\n",
        "  * We can read-in some previously computed noise properties from single-pulsar white noise analyses. These are things like `EFAC`, `EQUAD`, and (for `NANOGrav`) `ECORR`.\n",
        "\n",
        "  * In practice, we set these white-noise properties as fixed in the low-frequency noise / GW searches to reduce the computational cost of the analysis significantly.\n",
        "\n",
        "  * The noise properties have been stored as `json` files, and are read into a big parameter dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4AriF_9XGiu"
      },
      "outputs": [],
      "source": [
        "## Get parameter noise dictionary\n",
        "noise_ng15 = datadir + '/15yr_wn_dict.json'\n",
        "\n",
        "wn_params = {}\n",
        "with open(noise_ng15, 'r') as fp:\n",
        "    wn_params.update(json.load(fp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8VDJWy5XGiu"
      },
      "source": [
        "## Set up `enterprise` model for PTA GWB search\n",
        "\n",
        "  * Here we use `enterprise` directly to describe how we build the GWB model\n",
        "  \n",
        "  * Afterward, we will use `enterprise_extensions` for common models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCBsCyXGXGiu"
      },
      "outputs": [],
      "source": [
        "# find the maximum time span to set GW frequency sampling\n",
        "tmin = [p.toas.min() for p in psrs]\n",
        "tmax = [p.toas.max() for p in psrs]\n",
        "Tspan = np.max(tmax) - np.min(tmin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW7yIm4qXGiu"
      },
      "source": [
        "* We use the `Selection` object to define which noise parameters are assigned to which chunks of TOAs.\n",
        "\n",
        "* This selection is based on unique combinations of backends and receivers, for example, GUPPI at Green Bank or PUPPI at Arecibo.\n",
        "* For more information, see the [NANOGrav glossary](https://nanograv.org/glossary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk-5R604XGiu"
      },
      "outputs": [],
      "source": [
        "# define selection by observing backend\n",
        "selection = selections.Selection(selections.by_backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcimekDmXGiu"
      },
      "source": [
        "### Priors\n",
        "\n",
        "* White noise is fixed by using `Constant` parameters.\n",
        "\n",
        "* In this case we do not specify a default value for all instances of that parameter but instead will set them (based on their initialized pulsar and backend specific name) later via the `set_default_params` method of our `PTA` object.\n",
        "\n",
        "* For detection analyses we use a `Uniform` prior on the log of the GWB parameters and intrinsic red noise parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrG-9KJ2XGiu"
      },
      "outputs": [],
      "source": [
        "# white noise parameters\n",
        "efac = parameter.Constant()\n",
        "t2equad = parameter.Constant()\n",
        "ecorr = parameter.Constant()\n",
        "# we'll set these later with the wn_params dictionary\n",
        "\n",
        "# red noise parameters\n",
        "log10_A = parameter.Uniform(-20, -11)\n",
        "gamma = parameter.Uniform(0, 7)\n",
        "\n",
        "# GW parameters (initialize with names here to use parameters in common across pulsars)\n",
        "log10_A_gw = parameter.Uniform(-18, -14)('log10_A_gw')\n",
        "gamma_gw = parameter.Uniform(0, 7)('gamma_gw')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_cGWwV5XGiu"
      },
      "source": [
        "### Signals\n",
        "\n",
        "* With the priors specified, we now attach them to their associated signals.\n",
        "\n",
        "* In this tutorial we work with red noise with a power law spectrum $\\rho(f) = \\frac{A^2}{12 \\pi^2} \\frac{1}{T}\\left(\\frac{f_i}{f_\\mathrm{ref}}\\right)^{-\\gamma} \\mathrm{yr}^2$\n",
        "\n",
        "* Intrinsic red noise signals are expanded in a Fourier series with frequency bins at $f_i = i/T$ where $i = 1, 2, \\ldots, 30$\n",
        "\n",
        "* 30 is where we choose to stop and not a hard cutoff. This is enough to get the low-frequency content of the data.\n",
        "\n",
        "* For the GWB, we search over 14 frequencies, because the 14th bin is where a \"power law plus bend\" model turns up, indicating that the spectrum becomes flatter with more white noise instead of red noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_W_BboOXGiu"
      },
      "source": [
        "* First we will assume that this red noise is **uncorrelated** between pulsars, but later we will add in correlations which means that the pulse times of arrival in correlated models are synchronized in some way (either the model supports the pulses arriving together or apart, depending on the size of the angle between the pulsars on the sky).\n",
        "\n",
        "* There are many possible correlations that we could check against our data. In particular, there are good reasons to check Hellings and Downs correlations which would indicate a GWB, and monopolar and dipolar correlations which may indicate issues with clocks or Solar System ephemerides.\n",
        "\n",
        "* These exist in both `enterprise` and `enterprise_extensions` as **overlap reduction functions** (ORFs)\n",
        "\n",
        "* If we want to search for a common uncorrelated red noise (CURN) signal among all pulsars, we use `FourierBasisGP`\n",
        "\n",
        "* To add in Hellings and Downs (HD) correlations, a detection of which would be indicative of a GWB, we use `FourierBasisCommonGP` and input an `orf`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnLM82qYXGiu"
      },
      "source": [
        "* Finally, the timing model includes a description of when every pulse from the pulsar will arrive\n",
        "\n",
        "* We set `use_svd=True` to stabilize the timing model\n",
        "\n",
        "* Timing model uncertainties are marginalized analytically. There are a couple of methods to do this, and the `MarginalizingTimingModel` will be faster when we don't vary the white noise parameters. So we use `MarginalizingTimingModel` instead of `TimingModel`. Note that both of these marginalize over the timing model, but one does this in two steps instead of one. See https://arxiv.org/abs/2306.16223 for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP97TjHzXGiu"
      },
      "outputs": [],
      "source": [
        "# white noise\n",
        "mn = white_signals.MeasurementNoise(efac=efac, log10_t2equad=t2equad, selection=selection)\n",
        "ec = white_signals.EcorrKernelNoise(log10_ecorr=ecorr, selection=selection)\n",
        "\n",
        "# red noise (powerlaw with 30 frequencies)\n",
        "pl = utils.powerlaw(log10_A=log10_A, gamma=gamma)\n",
        "rn = gp_signals.FourierBasisGP(spectrum=pl, components=30, Tspan=Tspan)\n",
        "\n",
        "# gwb (no spatial correlations)\n",
        "cpl = utils.powerlaw(log10_A=log10_A_gw, gamma=gamma_gw)\n",
        "\n",
        "# change from 30 to 14 frequencies to line up with the 15 year GWB search\n",
        "curn = gp_signals.FourierBasisGP(spectrum=cpl, components=14, Tspan=Tspan, name='gw')\n",
        "\n",
        "# for HD spatial correlations you can do...\n",
        "# Note that this model is very slow compared to the CURN model above\n",
        "# orf=utils.hd_orf()\n",
        "# gwb = gp_signals.FourierBasisCommonGP(cpl, orf=utils.hd_orf(), components=14, Tspan=Tspan, name='gw')\n",
        "\n",
        "# timing model\n",
        "tm = gp_signals.MarginalizingTimingModel(use_svd=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVLlMhNLXGiu"
      },
      "source": [
        "### Create the full model\n",
        "\n",
        "* In `enterprise`, we can add signals together and use them on a `Pulsar` object to make individual pulsar models.\n",
        "\n",
        "* Calling `signal_base.PTA` on this list of pulsar models yields the final `PTA` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXLocSf4XGiu"
      },
      "outputs": [],
      "source": [
        "s = tm + mn + ec + rn + curn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0GSITZIXGiu"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "\n",
        "for p in psrs:\n",
        "    models.append(s(p))\n",
        "\n",
        "pta = signal_base.PTA(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY8ZfAdMXGiu"
      },
      "source": [
        "### Set the constant white noise parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcxBhWfNXGiu"
      },
      "outputs": [],
      "source": [
        "pta.set_default_params(wn_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UYUB9f-XGiu"
      },
      "source": [
        "## Sample!\n",
        "\n",
        "* With the PTA model in hand and with our constant parameters set to specific values, we are now ready to sample!\n",
        "\n",
        "* NANOGrav's favorite sampler is `PTMCMCSampler`, a parallel tempering enabled Markov chain Monte Carlo sampler\n",
        "\n",
        "* Stochastic sampling allows us to approximate the posterior distribution given by multiplying our likelihood and priors together. As is common in statistics, we use the log likelihood and prior instead.\n",
        "\n",
        "* This stochastic sampler, uses an adaptive Metropolis-Hastings algorithm to sample the $\\mathcal{O}(100)$ dimensional PTA parameter space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "logAQKaOXGiu"
      },
      "outputs": [],
      "source": [
        "# set initial parameters drawn from prior\n",
        "x0 = np.hstack([p.sample() for p in pta.params])\n",
        "ndim = len(x0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4a-nNtSXGiu"
      },
      "outputs": [],
      "source": [
        "# set up the sampler:\n",
        "# initial jump sample covariance matrix\n",
        "# this helps decide which point is chosen next and the scale of the problem\n",
        "cov = np.diag(np.ones(ndim) * 0.01**2)\n",
        "\n",
        "# set the location to save the output chains\n",
        "if IN_COLAB:\n",
        "    outDir = '/content/15yr_stochastic_analysis/tutorials/chains/pe_chains'\n",
        "else:\n",
        "    outDir = './chains/pe_chains'\n",
        "\n",
        "sampler = ptmcmc(ndim, pta.get_lnlikelihood, pta.get_lnprior, cov,\n",
        "                 outDir=outDir, resume=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqxxiGmDXGiu"
      },
      "source": [
        "### Empirical distributions\n",
        "\n",
        "* Because of the high-dimensional parameter space, it can take a very long time for chains to find the \"typical set\" of the posterior. This just means the part of the parameter space that the sampler spends the most time in.\n",
        "\n",
        "* This segment of finding the typical set is often called the \"burn-in\".\n",
        "\n",
        "* To significantly reduce the burn-in, we can propose samples from 2D histograms of the red-noise parameter posteriors from noise runs performed on individual pulsars. These are often close enough to the posterior when also including a GWB that this speeds up the burn-in time significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4JsJN7lXGiu"
      },
      "outputs": [],
      "source": [
        "with open(datadir + '/15yr_emp_distr.json', 'r') as fp:\n",
        "    emp_dists = json.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHjszwFxXGiu"
      },
      "outputs": [],
      "source": [
        "emp_distr = []\n",
        "for key in emp_dists.keys():\n",
        "    emp_distr.append(EmpiricalDistribution2D(emp_dists[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WEm1qEaXGiu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# add empirical distribution to sampler:\n",
        "jp = ee_sampler.JumpProposal(pta, empirical_distr=emp_distr)\n",
        "sampler.addProposalToCycle(jp.draw_from_empirical_distr, 10)  # add empirical distribution draws with weight 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pxA5wdeXGiu"
      },
      "source": [
        "### Sample!\n",
        "\n",
        "* Doing this sampling takes many hours to get an adequate amount of samples to approximate the posterior. If we ask for 5 million samples, the output will be 500,000 samples. The reason for this is to reduce the amount of space required for the chain and to reduce the autocorrelation between chain samples (this will reduce it by a factor of 10). Independent samples quantify the uncertainty on the approximation to the posterior that we get at the end of sampling.\n",
        "\n",
        "* While you are welcome to do this yourself, we have included a full set of samples from the posterior\n",
        "\n",
        "* We can post process these precomputed chains and discuss the results immediately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Yt5rXrXGiu"
      },
      "outputs": [],
      "source": [
        "# sampler for N iterations (uncomment the last line if you want to sample this yourself)\n",
        "# PTMCMCSampler thins the chain by default, so we get 1/10th of the samples we ask for\n",
        "N = int(5e6)\n",
        "x0 = np.hstack([p.sample() for p in pta.params])\n",
        "# sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlIEEW2PXGiv"
      },
      "source": [
        "### Post-process with `la_forge`\n",
        "\n",
        "* `la_forge` uses custom `Core` objects to store MCMC chains from `PTMCMCSampler` into `HDF5` file formats.\n",
        "\n",
        "* This compresses the chains into a small enough size that they can be easily transferred.\n",
        "\n",
        "* Here we will look at the `Core` that results from the above model. This model involves a common uncorrelated red noise (CURN) process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy1q5kgiXGiv"
      },
      "outputs": [],
      "source": [
        "# To load your own chains, include the chain folder's path in core.Core()\n",
        "# sampled_core = core.Core(outDir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPQlyI4fXGiv"
      },
      "outputs": [],
      "source": [
        "# set the location to load presampled chains from\n",
        "if IN_COLAB:\n",
        "    presampled = '/content/15yr_stochastic_analysis/tutorials/presampled_cores'\n",
        "else:\n",
        "    presampled = './presampled_cores'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJOyH5wvXGiv"
      },
      "outputs": [],
      "source": [
        "crn_core = core.Core(corepath=presampled + '/curn_14f_pl_vg.core')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxGHny42XGiv"
      },
      "source": [
        "* Using the `diagnostics` module of `la_forge`, we can test that the chains stay in one place over the duration of the chain with the Gelman-Rubin $\\hat{R}$ diagnostic using `grubin`.\n",
        "\n",
        "* This is one method to ensure that convergence has been reached (although it is not guaranteed by this test!). It splits the chain into multiple pieces and compares the within and between chain segment variances to decide whether the chain is stationary.\n",
        "\n",
        "* Additionally, we can check the number of effective samples in the chain with `plot_neff` or plot the marginalized posteriors with `plot_chains`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrJMyQ88XGiv"
      },
      "outputs": [],
      "source": [
        "# A threshold of 1.01 suggests that the chain has reached a stationary state\n",
        "diagnostics.plot_grubin(crn_core)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrz9YwbKXGiv"
      },
      "outputs": [],
      "source": [
        "diagnostics.plot_chains(crn_core, pars=['gw_gamma', 'gw_log10_A'], ncols=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6xqZT80XGiv"
      },
      "source": [
        "* To plot the joint posterior for these two parameters, we use `corner`\n",
        "\n",
        "* Typically, the reference frequency ($f_\\mathrm{ref}$ in the cell above) is set to $1/\\mathrm{ yr}$, but in this case, we also plot $1/(10\\mathrm{yr})$\n",
        "\n",
        "* When the reference frequency is set to $\\mathrm{yr}^{-1}$, the amplitude ($\\log_{10}A$) and spectral index ($\\gamma$) appear correlated, but this disappears when we set $f_{\\mathrm{ref}}$ to a more sensitive frequency.\n",
        "\n",
        "* PTAs are not sensitive to frequencies of $\\mathrm{yr}^{-1}$ due to fitting of proper motion and pulsar position parameters in the timing model\n",
        "\n",
        "* A vertical line at $\\gamma = 13/3$ indicates the spectral index that is expected for an ensemble of supermassive black hole binaries emitting gravitational waves. Lower $\\gamma$ could result from environmental effects nearby these binaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUdhVi4EXGiv"
      },
      "outputs": [],
      "source": [
        "joint_1yr = np.array([crn_core.get_param('gw_gamma'), crn_core.get_param('gw_log10_A')]).T\n",
        "\n",
        "def joint_correction(yrs):\n",
        "    correction = 0.5 * (3 - crn_core.get_param('gw_gamma')) * np.log10(1/yrs)\n",
        "    joint = correction + crn_core.get_param('gw_log10_A')\n",
        "    return joint\n",
        "\n",
        "# change 10 in the next line to the number of years you want to see how it affects the correlation!\n",
        "joint_10yr = np.array([crn_core.get_param('gw_gamma'), joint_correction(10)]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT_k18NUXGiv"
      },
      "outputs": [],
      "source": [
        "fig = corner.corner(joint_1yr, truths=[13/3, None], bins=30, color='black', labels=[r'$\\gamma$', r'$\\log_{10}A$'])\n",
        "plt.plot([], [], color='black', label=r'$\\mathrm{yr}^{-1}$')\n",
        "plt.plot([], [], color='C3', label=r'$.1\\mathrm{yr}^{-1}$')\n",
        "corner.corner(joint_10yr, bins=30, color='C3', fig=fig)\n",
        "plt.legend(loc='upper left', fontsize=8)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgVQDeBmXGiv"
      },
      "source": [
        "* The maximum posterior amplitude depends on the $f_\\mathrm{ref}$ that we choose for our model.\n",
        "\n",
        "* $\\gamma = 4.33$ falls at the edge of the posterior. While we have a physical reason to suspect that $\\gamma$ might fall on or near this value, our model is only marginally consistent with it.\n",
        "\n",
        "* If we change the model for dispersion measure variations from a piecewise constant (DMX) to a Gaussian process (DMGP), we find that that model has more support for this value. That is, the values supported by our posterior is dependent on the noise models, and we should wait and see how this changes with further observations and data.\n",
        "\n",
        "* **Finding a value that is not $13/3$ doesn't imply that anything is wrong with the model.** We can't know *a priori* what value this $\\gamma$ takes in the real world or even if the power law model is the correct model to use in the first place. All we can say is \"if we use a power law model on the  (with other underlying assumptions, such as stationary noise, etc.), then we find this posterior distribution of $\\gamma$ values.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt7KlG2dXGiv"
      },
      "source": [
        "## Free spectrum analysis\n",
        "\n",
        "* This model includes 30 frequency bins at $f_i / T$ and allows each of the frequency bins to be varied individually.\n",
        "\n",
        "* To create such a model, we will use `enterprise_extensions` to easily create common models.\n",
        "\n",
        "* `model_general` takes several arguements, the ones which we use here are:\n",
        "  * psrs: pulsars we want in the PTA\n",
        "  * noisedict: white noise parameters to be set for constant parameters\n",
        "  * common_components: number of frequencies to use in the common signals\n",
        "  * tm_svd: use SVD on the timing model design matrix (a good idea in general)\n",
        "  * orf: overlap reduction function (correlation function such as HD correlations)\n",
        "  * common_psd: model used for the common signals\n",
        "  * tm_marg: use two-step marginalization (faster when we have fixed white noise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nheoD4eWXGiv"
      },
      "outputs": [],
      "source": [
        "pta = model_general(psrs, noisedict=wn_params, common_components=30, tm_svd=True,\n",
        "                    orf='hd', common_psd='spectrum', tm_marg=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg7estHLXGiv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "outdir = './chains/pe_free_spec'\n",
        "sampler = ee_sampler.setup_sampler(pta, outdir=outdir, empirical_distr=emp_distr, resume=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql3tTPOAXGiv"
      },
      "outputs": [],
      "source": [
        "# sampler for N iterations (uncomment the last line if you want to sample this yourself)\n",
        "# PTMCMCSampler thins the chain by default, so we get 1/10th of the samples we ask for\n",
        "N = int(5e6)\n",
        "x0 = np.hstack([p.sample() for p in pta.params])\n",
        "# sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfERpABsXGiv"
      },
      "source": [
        "* Here we load both the free spectral model and a power law model so that we can compare the preferred models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dF4RyhPXGiv"
      },
      "outputs": [],
      "source": [
        "hd_fs_core = core.Core(corepath=presampled + '/hd_30f_fs.core')\n",
        "hd_pl_core = core.Core(corepath=presampled + '/hd_14f_pl_vg.core')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGPwIS6NXGiv"
      },
      "outputs": [],
      "source": [
        "# frequencies in 15yr dataset\n",
        "Tspan = 505861299.1401644\n",
        "freqs = np.arange(1,31)/Tspan\n",
        "\n",
        "hd_samples = [hd_fs_core.get_param('gw_hd_log10_rho_{}'.format(ii)) for ii in range(30)]\n",
        "median_log10_A = np.median(hd_pl_core.get_param('gw__log10_A'))\n",
        "median_gamma = np.median(hd_pl_core.get_param('gw__gamma'))\n",
        "\n",
        "pl_med = utils.powerlaw(f=np.repeat(freqs,2), log10_A=median_log10_A, gamma=median_gamma)[::2]\n",
        "\n",
        "print('Median log10_A:', median_log10_A)\n",
        "print('Median gamma:', median_gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "590v8CcsXGiv"
      },
      "source": [
        "* We are now ready to plot the individual bins of the free spectral model. Only the first 10 bins are shown in the following plot for clarity. This can easily be changed by changing `ax.set_xlim` to include values further to the right.\n",
        "\n",
        "* For this we use a violin plot. A violin plot is a plot of several marginalized parameter posterior distributions. In this case, we plot the posterior for each of the bins at their frequencies. Thinner \"stems\" of the violins imply stronger detections of excess timing delays in pulse TOAs at these frequencies.\n",
        "\n",
        "* As we can see here, the median power law fits reasonably well along the individual bins of the free spectral model. It goes from the center of the distribution at the lowest frequency, through the portions with support in the next three bins, and then through several bins that do not have strong detections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqjK6NcMXGiv"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, rasterized=True)\n",
        "\n",
        "# plot violins\n",
        "v1 = ax.violinplot(hd_samples, positions=np.log10(freqs),\n",
        "                   widths=0.04, showextrema=False)\n",
        "# label the HD spectrum\n",
        "plt.plot([], [], color='C0', linestyle='solid',\n",
        "         label='Hellings-Downs Spectrum', alpha=0.5)\n",
        "\n",
        "# Make the violins look good B)\n",
        "for pc in v1['bodies']:\n",
        "    pc.set_facecolor('None')\n",
        "    pc.set_edgecolor('C0')\n",
        "    pc.set_linestyle('solid')\n",
        "    pc.set_alpha(0.5)\n",
        "    pc.set_linewidth(1.5)\n",
        "\n",
        "ax.set_ylim(-8.9, -5)\n",
        "ax.set_xlim(-8.8, -7.68)\n",
        "ax.grid(which='both',alpha=0.1)\n",
        "# plot the median power law\n",
        "ax.plot(np.log10(freqs), 0.5*np.log10(pl_med), color='C1', alpha=0.5, lw=2, label='Median Varied Gamma PL')\n",
        "\n",
        "ax.minorticks_on()\n",
        "ax.tick_params(which='both', direction='in', tick2On=True)\n",
        "\n",
        "plt.xticks(fontsize=12, rotation=45)\n",
        "plt.yticks(fontsize=12, rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "ax.set_ylabel(r'$\\log_{10}$(Excess timing delay [s])')\n",
        "ax.set_xlabel(r'$\\log_{10}$(Frequency [Hz])')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYIIDzVoXGiv"
      },
      "source": [
        "## Spline ORF Fit\n",
        "\n",
        "* To fit for HD correlations with Bayesian models, we use the HD correlated model. If we want to see this visually, we can use a spline fit to the ORF. First, we split the pulsars up into bins containing pulsar pairs with angles of separation on the sky.\n",
        "\n",
        "* There are seven spline knots at `[1e-3, 25.0, 49.3, 82.5, 121.8, 150.0, 180.0]` degrees each indicating that pulsar pairs in these bins will have a separation angle which is near each value.\n",
        "\n",
        "* The vertical value at each of the $7$ values is allowed to vary and is sampled over with `PTMCMCSampler`\n",
        "\n",
        "* To use this model, we choose the `orf=spline_orf` in `model_general`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqOZmVlqXGiv"
      },
      "outputs": [],
      "source": [
        "pta = model_general(psrs, noisedict=wn_params, common_components=14, tm_svd=True,\n",
        "                    orf='spline_orf', common_psd='powerlaw', tm_marg=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vlXKpacXGiv"
      },
      "outputs": [],
      "source": [
        "# gw spline parameters plus the powerlaw parameters\n",
        "pta.params[134:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1O7-gQDXGiw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "outdir = './chains/pe_splines'\n",
        "sampler = ee_sampler.setup_sampler(pta, outdir=outdir, empirical_distr=emp_distr, resume=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48L3ST6nXGiw"
      },
      "outputs": [],
      "source": [
        "# sampler for N iterations (uncomment the last line if you want to sample this yourself)\n",
        "# PTMCMCSampler thins the chain by default, so we get 1/10th of the samples we ask for\n",
        "N = int(5e6)\n",
        "x0 = np.hstack([p.sample() for p in pta.params])\n",
        "pars = pta.param_names\n",
        "idx_orf_params = [list(pars).index(pp) for pp in pars if 'gw_orf_bin' in pp]\n",
        "\n",
        "# start with an initial value given here for the spline parameters\n",
        "x0[idx_orf_params] = [.01 for x in range(len(idx_orf_params))]\n",
        "\n",
        "# sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOicm_FaXGiw"
      },
      "outputs": [],
      "source": [
        "spline_core = core.Core(corepath=presampled + '/spline_orf_vg.core')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XElIumYjXGiw"
      },
      "outputs": [],
      "source": [
        "violins = [spline_core.get_param('gw_orf_spline_{}'.format(ii)) for ii in range(7)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei8GcwBXXGiw"
      },
      "outputs": [],
      "source": [
        "pos =  np.array([1e-3, 25.0, 49.3, 82.5,\n",
        "                         121.8, 150.0, 180.0])\n",
        "q = [[.158, 0.5, 0.84] for _ in range(len(pos))]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, rasterized=True)\n",
        "v1 = plt.violinplot(violins,positions=pos,\n",
        "                   quantiles=q, bw_method=0.3,\n",
        "                    widths=15.0, showextrema=False)\n",
        "\n",
        "for pc in v1['bodies']:\n",
        "    pc.set_facecolor('None')\n",
        "    pc.set_edgecolor('C0')\n",
        "    pc.set_linestyle('None')\n",
        "    pc.set_alpha(1.0)\n",
        "\n",
        "def hd_orf(xi):\n",
        "    omc2 = (1 - np.cos(xi)) / 2\n",
        "    return 1.5 * omc2 * np.log(omc2) - 0.25 * omc2 + 0.5\n",
        "\n",
        "ax.plot(np.linspace(1e-3, np.pi,1000)*180.0/np.pi,\n",
        "        hd_orf(np.linspace(1e-3,np.pi,1000)),\n",
        "        lw = 1.5, color='k', ls = '--')\n",
        "ax.axhline(0, color='k', ls='-.', lw=0.8)\n",
        "\n",
        "\n",
        "plt.xticks(labels = ['0', '30', '60', '90', '120', '150', '180'],\n",
        "           ticks = np.array([0, 30, 60, 90, 120, 150, 180]))\n",
        "plt.xlabel(r'Separation Angle Between Pulsars, $\\xi_{ab}$ [degrees]',fontsize=10)\n",
        "plt.ylabel(r'$\\Gamma(\\xi_{ab})$')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMATtt03XGiw"
      },
      "source": [
        "* The HD curve plotted here as a dashed, black line fits reasonably well to the spline orf\n",
        "\n",
        "* We use $7$ bins here, but we could have used any number of bins. We should find that the results are similar and that there is a consistent fit to the curve for each bin used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozNL14VvXGiw"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "* This concludes the parameter estimation tutorial\n",
        "\n",
        "* This covers $3/4$ of the plots in Figure 1 of the NANOGrav 15-year GWB paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eVGRJEAXGiw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "enterprise_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}